# llm-manager configuration
#
# Per task model configuration for Ollama.
# The default models here are relatively small and should be possible to run on many laptops, also ones without capable GPUs.

# For chatting
chat=llama3.2:3b

# For code completion / tab autocompletion
code-completion=deepseek-coder:1.3b

# A small model, for quick tests
test=tinyllama:1b

# Text generation
text-generation=gemma2:2b

# Tool use and function calling
tool-use=llama3.2:3b

# For translating text (not single words, though)
translation=mixtral:8x7b

# Vision and image descriptions
vision=llava-llama3:8b
